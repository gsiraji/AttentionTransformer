{"cells":[{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# import the libraries\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","import math, copy, time\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sn "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create the encoder-decoder class\n","\n","class EncoderDecoder(nn.Module):\n","    \"\"\"\n","    Base class for the model\n","    encoder maps input seq to a representation\n","    decoder maps rep to an output seq\n","    \"\"\"\n","\n","    def __init__(self, encoder, decoder, source_embed,\n","                  target_embed, generator):\n","        super(EncoderDecoder, self).__init__\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.source_embed = source_embed\n","        self.target_embed = target_embed\n","        self.generator = generator\n","\n","    def forward(self, source, target, \n","                source_mask, target_mask):\n","        \n","        return self.decode(self.encode(source, source_mask),\n","                           source_mask, target,target_mask)\n","    \n","    def encode(self,source,source_mask):\n","        return self.decoder(self.source_embed(source), source_mask)\n","    \n","    def decode(self, memory, source_mask, target, target_mask):\n","        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)\n","    \n","\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Generator(nn.Module):\n","     \"\"\"\n","    Base generator class for the model\n","    perform linear and log softmax\n","    \"\"\"\n","     \n","     def __init__(self, d_model, vocabulary) -> None:\n","          super(Generator, self).__init__()\n","          self.project = nn.Linear(d_model, vocabulary)\n","\n","     def forward(self,x):\n","          # log(exp(x_i)/sum_j(exp(x_j)))\n","          return F.log_softmax(self.project(x), dim=-1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The Encoder Stack\n","\n","# create 6 identical layers for the encoder\n","\n","def clone(layer, number_of_layers):\n","    \"make number_of_layers clones of a layer\"\n","    return nn.ModuleList([copy.deepcopy(layer) for _ in range(number_of_layers)])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class Encoder(nn.Module):\n","    \"Core encoder is the number_of_layers identical layers\"\n","    def __init__(self, layer,number_of_layers):\n","        super(Encoder, self).__init__()\n","        self.layers = clone(layer, number_of_layers)\n","        # norm = (x-E[x])/sqrt(Var[x])*gamma + beta \n","        self.norm = nn.LayerNorm(layer.size)\n","\n","\n","    def forward(self,x,mask):\n","        \"Forward pass the input x and its mask\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x) "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    \"Construct custom LayerNorm with learnable parameters gamma and beta\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(features))\n","        self.beta = nn.Parameter(torch.zeros(features))\n","        # note that the default eps in pytorch is 1e-5\n","        # here it's set to 1e-6 \n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","\n","        return self.gamma * (x - mean) / (std + self.eps) + self.beta"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    Calculates x + Dropout(Sublayer(Norm(x)))\n","    Residual connection \n","    \"\"\"\n","\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Applies residual connection to sublayers with the same size \"\n","        return x + self.dropout(sublayer(self.norm(x)))\n","    \n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Each layer has 2 sublayers, a self-attention layer and a fully connected feed forward one."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EncodeLayer(nn.Module):\n","    \"\"\"\n","    This layer has 2 sublayers: self-attention and forward-feed \n","    \"\"\"\n","\n","    def __init__(self, size, dropout, self_attention, feed_forward):\n","        super(EncodeLayer, self).__init__()\n","        self.self_attention = self_attention\n","        self.feed_forward = feed_forward\n","        self.sublayer = clone(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Decoder: decoder has 6 identical layers "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Decoder(nn.Module):\n","     \"\"\"\n","    Decoder with number_of_layers layers and mask\n","    \"\"\"\n","     \n","     def __init__(self, layer, number_of_layers):\n","          super(Decoder, self).__init__()\n","\n","          self.layers = clone(layer, number_of_layers)\n","          self.norm = LayerNorm(layer.size)\n","\n","     def forward(self, x, memory, source_mask, target_mask):\n","          for layer in self.layers:\n","               x = layer(x, memory, source_mask, target_mask)\n","          return self.norm(x)\n","     \n","     "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
